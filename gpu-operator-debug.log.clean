=== CLEANED DEBUG LOG ===

=== Pod Status ===
NAME                                                          READY   STATUS                  RESTARTS      AGE     IP            NODE                 NOMINATED NODE   READINESS GATES
gpu-feature-discovery-mqsbg                                   0/1     Init:0/2                0             3m16s   10.244.0.16   kind-control-plane   <none>           <none>
gpu-operator-669887d96d-pfrbb                                 1/1     Running                 0             3m35s   10.244.0.6    kind-control-plane   <none>           <none>
gpu-operator-node-feature-discovery-gc-76dc6664b8-hz97j       1/1     Running                 0             3m35s   10.244.0.7    kind-control-plane   <none>           <none>
gpu-operator-node-feature-discovery-master-7d6b448f6d-g7lpm   1/1     Running                 0             3m35s   10.244.0.5    kind-control-plane   <none>           <none>
gpu-operator-node-feature-discovery-worker-rlnlp              1/1     Running                 0             3m35s   10.244.0.8    kind-control-plane   <none>           <none>
nvidia-container-toolkit-daemonset-6h6m7                      1/1     Running                 0             3m17s   10.244.0.9    kind-control-plane   <none>           <none>
nvidia-dcgm-exporter-9ch9d                                    0/1     Init:0/1                0             3m16s   10.244.0.19   kind-control-plane   <none>           <none>
nvidia-device-plugin-daemonset-f7lgq                          0/1     Init:0/1                0             3m17s   10.244.0.18   kind-control-plane   <none>           <none>
nvidia-operator-validator-s6vzt                               0/1     Init:CrashLoopBackOff   4 (75s ago)   3m17s   10.244.0.17   kind-control-plane   <none>           <none>


=== Unique Error Messages ===
Error from server (BadRequest): container "cuda-validation" in pod "nvidia-operator-validator-s6vzt" is waiting to start: PodInitializing
Error from server (BadRequest): container "gpu-feature-discovery-imex-init" in pod "gpu-feature-discovery-mqsbg" is waiting to start: PodInitializing
Error from server (BadRequest): container "nvidia-dcgm-exporter" in pod "nvidia-dcgm-exporter-9ch9d" is waiting to start: PodInitializing
Error from server (BadRequest): container "nvidia-device-plugin" in pod "nvidia-device-plugin-daemonset-f7lgq" is waiting to start: PodInitializing
Nov 02 01:32:00 kind-control-plane containerd[6149]: time="2024-11-02T01:32:00.980114211Z" level=warning msg="cleaning up after shim disconnected" id=68d9497c4cd0991432c7a82fa93ac2fa8afb588ed933d866747809996447267c namespace=k8s.io
Nov 02 01:32:01 kind-control-plane containerd[6149]: time="2024-11-02T01:32:01.849580997Z" level=error msg="RemoveContainer for \"9064123dfcf6db793ba036533a599a3705187e97d6ede8596ab4b177ae2a0719\" failed" error="failed to set removing state for container \"9064123dfcf6db793ba036533a599a3705187e97d6ede8596ab4b177ae2a0719\": container is already in removing state"
[pod/nvidia-container-toolkit-daemonset-6h6m7/driver-validation] time="2024-11-02T01:30:19Z" level=warning msg="Could not create symlink: symlink /dev/nvidia-caps/nvidia-cap1 /host-dev-char/238:1: file exists"
[pod/nvidia-container-toolkit-daemonset-6h6m7/driver-validation] time="2024-11-02T01:30:19Z" level=warning msg="Could not create symlink: symlink /dev/nvidia-caps/nvidia-cap2 /host-dev-char/238:2: file exists"
[pod/nvidia-container-toolkit-daemonset-6h6m7/driver-validation] time="2024-11-02T01:30:19Z" level=warning msg="Could not create symlink: symlink /dev/nvidia-uvm /host-dev-char/235:0: file exists"
[pod/nvidia-container-toolkit-daemonset-6h6m7/driver-validation] time="2024-11-02T01:30:19Z" level=warning msg="Could not create symlink: symlink /dev/nvidia-uvm-tools /host-dev-char/235:1: file exists"
[pod/nvidia-container-toolkit-daemonset-6h6m7/driver-validation] time="2024-11-02T01:30:19Z" level=warning msg="Could not create symlink: symlink /dev/nvidia0 /host-dev-char/195:0: file exists"
[pod/nvidia-container-toolkit-daemonset-6h6m7/driver-validation] time="2024-11-02T01:30:19Z" level=warning msg="Could not create symlink: symlink /dev/nvidia1 /host-dev-char/195:1: file exists"
[pod/nvidia-container-toolkit-daemonset-6h6m7/driver-validation] time="2024-11-02T01:30:19Z" level=warning msg="Could not create symlink: symlink /dev/nvidiactl /host-dev-char/195:255: file exists"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Skipping library candidate '/usr/lib/aarch64-linux-gnu/libnvidia-ml.so': error resolving link '/usr/lib/aarch64-linux-gnu/libnvidia-ml.so': lstat /usr/lib/aarch64-linux-gnu: no such file or directory"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Skipping library candidate '/usr/lib/x86_64-linux-gnu/libnvidia-ml.so': error resolving link '/usr/lib/x86_64-linux-gnu/libnvidia-ml.so': lstat /usr/lib/x86_64-linux-gnu/libnvidia-ml.so: no such file or directory"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Skipping library candidate '/usr/lib64/libnvidia-container-go.so.1': error resolving link '/usr/lib64/libnvidia-container-go.so.1': lstat /usr/lib64/libnvidia-container-go.so.1: no such file or directory"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Skipping library candidate '/usr/lib64/libnvidia-ml.so': error resolving link '/usr/lib64/libnvidia-ml.so': lstat /usr/lib64/libnvidia-ml.so: no such file or directory"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=warning msg="Error finding library path for root /: error locating NVIDIA management library: error locating library 'libnvidia-ml.so'"
[pod/nvidia-operator-validator-s6vzt/toolkit-validation] time="2024-11-02T01:32:00Z" level=info msg="Error: error validating toolkit installation: exit status 12"

=== NVIDIA Libraries ===

	libnvidia-glsi.so.535.104.12 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libnvidia-glsi.so.535.104.12
	libnvidia-glsi.so.535.183.01 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libnvidia-glsi.so.535.183.01
	libnvidia-glvkspirv.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libnvidia-glvkspirv.so
	libnvidia-glvkspirv.so.535.104.12 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libnvidia-glvkspirv.so.535.104.12
	libnvidia-glvkspirv.so.535.183.01 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libnvidia-glvkspirv.so.535.183.01
	libnvidia-ml.so.1 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1
--
-rwxr-xr-x 1 root root   1942744 May 12 19:30 /usr/lib/x86_64-linux-gnu/libnvidia-ml.so
-rwxr-xr-x 1 root root   1942744 May 12 19:30 /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1
-rwxr-xr-x 1 root root   1942744 May 12 19:30 /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.535.183.01
-rwxr-xr-x 1 root root   3322680 Sep 20  2023 /usr/lib/x86_64-linux-gnu/libnvidia-ngx.so.535.104.12
-rwxr-xr-x 1 root root   4553912 May 12 19:30 /usr/lib/x86_64-linux-gnu/libnvidia-ngx.so.535.183.01
=== GPU Feature Discovery Logs ===
Error from server (BadRequest): container "cuda-validation" in pod "nvidia-operator-validator-s6vzt" is waiting to start: PodInitializing
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Checking library candidate '/usr/lib/aarch64-linux-gnu/libnvidia-ml.so'"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Checking library candidate '/usr/lib/x86_64-linux-gnu/libnvidia-ml.so'"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Checking library candidate '/usr/lib64/libnvidia-ml.so'"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Finding library libnvidia-ml.so (root=/)"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Installed '/usr/local/nvidia/toolkit/nvidia-container-runtime.experimental.real'"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Installed wrapper '/usr/local/nvidia/toolkit/nvidia-container-runtime.experimental'"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Installing 'nvidia-container-runtime.experimental' to '/usr/local/nvidia/toolkit/nvidia-container-runtime.experimental.real'"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Installing executable 'nvidia-container-runtime.experimental' to /usr/local/nvidia/toolkit"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Skipping library candidate '/usr/lib/aarch64-linux-gnu/libnvidia-ml.so': error resolving link '/usr/lib/aarch64-linux-gnu/libnvidia-ml.so': lstat /usr/lib/aarch64-linux-gnu: no such file or directory"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Skipping library candidate '/usr/lib/x86_64-linux-gnu/libnvidia-ml.so': error resolving link '/usr/lib/x86_64-linux-gnu/libnvidia-ml.so': lstat /usr/lib/x86_64-linux-gnu/libnvidia-ml.so: no such file or directory"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Skipping library candidate '/usr/lib64/libnvidia-ml.so': error resolving link '/usr/lib64/libnvidia-ml.so': lstat /usr/lib64/libnvidia-ml.so: no such file or directory"
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=info msg="Using library root "
[pod/nvidia-container-toolkit-daemonset-6h6m7/nvidia-container-toolkit-ctr] time="2024-11-02T01:30:33Z" level=warning msg="Error finding library path for root /: error locating NVIDIA management library: error locating library 'libnvidia-ml.so'"
[pod/nvidia-operator-validator-s6vzt/toolkit-validation] NVIDIA-SMI couldn't find libnvidia-ml.so library in your system. Please make sure that the NVIDIA Display Driver is properly installed and present in your system.
[pod/nvidia-operator-validator-s6vzt/toolkit-validation] Please also try adding directory that contains libnvidia-ml.so to your system PATH.
[pod/nvidia-operator-validator-s6vzt/toolkit-validation] time="2024-11-02T01:32:00Z" level=info msg="Error: error validating toolkit installation: exit status 12"
[pod/nvidia-operator-validator-s6vzt/toolkit-validation] toolkit is not ready
lrwxrwxrwx 1 root root        27 May 13 01:12 /usr/lib/x86_64-linux-gnu/libnvidia-ngx.so.1 -> libnvidia-ngx.so.535.183.01
lrwxrwxrwx 1 root root        27 Nov  2 01:28 /usr/lib/x86_64-linux-gnu/libnvidia-ngx.so -> libnvidia-ngx.so.535.104.12
lrwxrwxrwx 1 root root        28 Nov  2 01:28 /usr/lib/x86_64-linux-gnu/libnvidia-nvvm.so -> libnvidia-nvvm.so.535.183.01

=== Containerd Config Highlights ===
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia]
          runtime_type = "io.containerd.runc.v2"

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia.options]
            BinaryName = "/usr/local/nvidia/toolkit/nvidia-container-runtime"

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia-cdi]
          runtime_type = "io.containerd.runc.v2"

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia-cdi.options]
            BinaryName = "/usr/local/nvidia/toolkit/nvidia-container-runtime.cdi"

=== Runtime Logs Highlights ===
=== Runtime Debug Logs ===
=== Runtime Debug Log ===
cat: /var/log/nvidia-container-runtime-debug.log: No such file or directory

=== CLI Debug Log ===
cat: /var/log/nvidia-container-cli-debug.log: No such file or directory


=== nvidia-smi Output ===
=== nvidia-smi ===
Sat Nov  2 01:33:15 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |

